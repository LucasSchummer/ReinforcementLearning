{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e34f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import numpy as np\n",
    "from agents.sac import SAC, ReplayBuffer\n",
    "from envs.panda_utils import generate_video, eval_model, save_plots\n",
    "from envs.utils import setup_training_dir\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570e0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada32489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('achieved_goal': Box(-10.0, 10.0, (3,), float32), 'desired_goal': Box(-10.0, 10.0, (3,), float32), 'observation': Box(-10.0, 10.0, (18,), float32))\n",
      "Box(-1.0, 1.0, (7,), float32)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"PandaPushJoints-v3\"\n",
    "version = \"v1\"\n",
    "max_episode_steps = 200\n",
    "env = gym.make(env_name, max_episode_steps=max_episode_steps)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1654bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False\n",
    "checkpoint = f\"training/sac/{env_name}/{version}/training2/100000.pth\"\n",
    "training_number = setup_training_dir(resume_training, \"sac\", env_name, version)\n",
    "\n",
    "max_training_time = 7 #h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46eaeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space['observation'].shape[0] + env.observation_space['desired_goal'].shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "buffer_size = 1000000\n",
    "max_timesteps = 1000000\n",
    "alpha = .2\n",
    "gamma = .99\n",
    "tau = .005\n",
    "lr = 3e-4\n",
    "use_her = True\n",
    "distance_threshold = .05\n",
    "batch_size = 256\n",
    "warmup_timesteps = 500\n",
    "eval_frequency = 2000\n",
    "n_episodes_eval = 10\n",
    "checkpoint_frequency = 100000\n",
    "video_frequency = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c1e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(state_size, n_actions, buffer_size, alpha, gamma, tau, lr, device, use_her, distance_threshold).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1905f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training:\n",
    "    training_vars = model.load_state(checkpoint)\n",
    "    timestep_start, avg_returns, avg_successes = training_vars\n",
    "else:\n",
    "    timestep_start = 0\n",
    "    avg_returns = []\n",
    "    avg_successes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4ce05c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     state_goal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mconcatenate([state, goal]))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timestep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     avg_return, avg_success \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage return after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     avg_returns\u001b[38;5;241m.\u001b[39mappend(avg_return)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\Coursera\\RL Specialization\\RL_applications\\ALE\\envs\\panda_utils.py:20\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(model, env_name, max_episode_steps, n_episodes_eval)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_model\u001b[39m(model, env_name, max_episode_steps, n_episodes_eval):\n\u001b[0;32m     19\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name, max_episode_steps\u001b[38;5;241m=\u001b[39mmax_episode_steps)\n\u001b[1;32m---> 20\u001b[0m     returns, successes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes_eval):\n\u001b[0;32m     22\u001b[0m         ep_return, success \u001b[38;5;241m=\u001b[39m run_eval_episode(env, model)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "state, goal = obs[\"observation\"], obs[\"desired_goal\"]\n",
    "state_goal = torch.tensor(np.concatenate([state, goal]))\n",
    "start_time = time.time()\n",
    "\n",
    "for timestep in range(timestep_start, max_timesteps):\n",
    "\n",
    "    action = model.act(state_goal)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    achieved_goal = obs[\"achieved_goal\"]\n",
    "    next_state = obs[\"observation\"]\n",
    "    done = terminated or truncated\n",
    "\n",
    "    model.save_to_buffer([state, goal, achieved_goal, action, reward, next_state, terminated])\n",
    "\n",
    "    if timestep >= warmup_timesteps: # Update every timestep after warmup\n",
    "        model.update(batch_size)\n",
    "\n",
    "    state = next_state\n",
    "    state_goal = torch.tensor(np.concatenate([state, goal]))\n",
    "\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "        state, goal = obs[\"observation\"], obs[\"desired_goal\"]\n",
    "        model.save_her_transitions(achieved_goal) # Create new HER transitions using the last achieved goal as the new target\n",
    "        state_goal = torch.tensor(np.concatenate([state, goal]))\n",
    "\n",
    "    if (timestep + 1) % eval_frequency == 0:\n",
    "        avg_return, avg_success = eval_model(model, env_name, max_episode_steps, n_episodes_eval)\n",
    "        print(f\"Average return after {timestep+1} timesteps : {avg_return}\")\n",
    "        avg_returns.append(avg_return)\n",
    "        avg_successes.append(avg_success)\n",
    "        save_plots(avg_returns, avg_successes, f\"training/sac/{env_name}/{version}/training{training_number}\", timestep+1, eval_frequency)\n",
    "\n",
    "    if (timestep + 1) % checkpoint_frequency == 0:\n",
    "        model.save_state(timestep, avg_returns, avg_successes, f\"training/sac/{env_name}/{version}/training{training_number}/{timestep+1}.pth\")\n",
    "\n",
    "    # if (timestep + 1) % video_frequency == 0:\n",
    "    #     generate_video(env_name, model, 1, \n",
    "    #            deterministic=True, \n",
    "    #            filename=f\"training/sac/{env_name}/{version}/training{training_number}/{timestep+1}.mp4\")\n",
    "        \n",
    "    if time.time() - start_time > 3600 * max_training_time:\n",
    "        print(f\"Maximum training time of {max_training_time}h exceeded. Interrupting training after {timestep} timesteps.\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b98c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6666666666666666)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_video(env_name, max_episode_steps, model, 10, random=False, deterministic=True,\n",
    "#                filename=f\"training/sac/{env_name}/{version}/training{training_number}/final.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c07806",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
