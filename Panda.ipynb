{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e34f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import numpy as np\n",
    "from agents.sac import SAC, ReplayBuffer\n",
    "from envs.panda_utils import generate_video, eval_model, save_plots\n",
    "from envs.utils import Normalizer, setup_training_dir\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570e0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada32489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('achieved_goal': Box(-10.0, 10.0, (3,), float32), 'desired_goal': Box(-10.0, 10.0, (3,), float32), 'observation': Box(-10.0, 10.0, (6,), float32))\n",
      "Box(-1.0, 1.0, (7,), float32)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"PandaReachJoints-v3\"\n",
    "version = \"v1\"\n",
    "max_episode_steps = 200\n",
    "env = gym.make(env_name, max_episode_steps=max_episode_steps)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1654bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False\n",
    "checkpoint = f\"training/sac/{env_name}/{version}/training2/100000.pth\"\n",
    "training_number = setup_training_dir(resume_training, \"sac\", env_name, version)\n",
    "\n",
    "max_training_time = 8 #h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46eaeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space['observation'].shape[0]\n",
    "goal_size = env.observation_space['desired_goal'].shape[0]\n",
    "state_goal_size = state_size + goal_size\n",
    "n_actions = env.action_space.shape[0]\n",
    "buffer_size = 1000000\n",
    "max_timesteps = 300000\n",
    "alpha = .2\n",
    "gamma = .99\n",
    "tau = .005\n",
    "lr = 3e-4\n",
    "use_her = False\n",
    "distance_threshold = .05\n",
    "batch_size = 256\n",
    "warmup_timesteps = 20000\n",
    "eval_frequency = 2000\n",
    "n_episodes_eval = 10\n",
    "checkpoint_frequency = 100000\n",
    "video_frequency = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c1e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(state_size, goal_size, n_actions, buffer_size, alpha, gamma, tau, lr, device, use_her, distance_threshold).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1905f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training:\n",
    "    training_vars = model.load_state(checkpoint)\n",
    "    timestep_start, avg_returns, avg_successes = training_vars\n",
    "else:\n",
    "    timestep_start = 0\n",
    "    avg_returns = []\n",
    "    avg_successes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ce05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return after 2000 timesteps : -183.8\n",
      "Average return after 4000 timesteps : -200.0\n",
      "Average return after 6000 timesteps : -200.0\n",
      "Average return after 8000 timesteps : -166.6\n",
      "Average return after 10000 timesteps : -184.3\n",
      "Average return after 12000 timesteps : -169.1\n",
      "Average return after 14000 timesteps : -185.0\n",
      "Average return after 16000 timesteps : -200.0\n",
      "Average return after 18000 timesteps : -162.5\n",
      "Average return after 20000 timesteps : -200.0\n",
      "Average return after 22000 timesteps : -200.0\n",
      "Average return after 24000 timesteps : -175.4\n",
      "Average return after 26000 timesteps : -200.0\n",
      "Average return after 28000 timesteps : -200.0\n",
      "Average return after 30000 timesteps : -175.4\n",
      "Average return after 32000 timesteps : -189.4\n",
      "Average return after 34000 timesteps : -200.0\n",
      "Average return after 36000 timesteps : -200.0\n",
      "Average return after 38000 timesteps : -200.0\n",
      "Average return after 40000 timesteps : -200.0\n",
      "Average return after 42000 timesteps : -200.0\n",
      "Average return after 44000 timesteps : -200.0\n",
      "Average return after 46000 timesteps : -200.0\n",
      "Average return after 48000 timesteps : -200.0\n",
      "Average return after 50000 timesteps : -200.0\n",
      "Average return after 52000 timesteps : -200.0\n",
      "Average return after 54000 timesteps : -200.0\n",
      "Average return after 56000 timesteps : -200.0\n",
      "Average return after 58000 timesteps : -200.0\n",
      "Average return after 60000 timesteps : -200.0\n",
      "Average return after 62000 timesteps : -200.0\n",
      "Average return after 64000 timesteps : -200.0\n",
      "Average return after 66000 timesteps : -200.0\n",
      "Average return after 68000 timesteps : -200.0\n",
      "Average return after 70000 timesteps : -181.5\n",
      "Average return after 72000 timesteps : -200.0\n",
      "Average return after 74000 timesteps : -181.4\n",
      "Average return after 76000 timesteps : -200.0\n",
      "Average return after 78000 timesteps : -200.0\n",
      "Average return after 80000 timesteps : -162.8\n",
      "Average return after 82000 timesteps : -181.6\n",
      "Average return after 84000 timesteps : -180.0\n",
      "Average return after 86000 timesteps : -143.8\n",
      "Average return after 88000 timesteps : -150.8\n",
      "Average return after 90000 timesteps : -160.7\n",
      "Average return after 92000 timesteps : -172.9\n",
      "Average return after 94000 timesteps : -163.4\n",
      "Average return after 96000 timesteps : -139.8\n",
      "Average return after 98000 timesteps : -155.5\n",
      "Average return after 100000 timesteps : -171.9\n",
      "Average return after 102000 timesteps : -130.0\n",
      "Average return after 104000 timesteps : -151.1\n",
      "Average return after 106000 timesteps : -145.5\n",
      "Average return after 108000 timesteps : -122.3\n",
      "Average return after 110000 timesteps : -143.6\n",
      "Average return after 112000 timesteps : -129.6\n",
      "Average return after 114000 timesteps : -182.1\n",
      "Average return after 116000 timesteps : -115.3\n",
      "Average return after 118000 timesteps : -102.1\n",
      "Average return after 120000 timesteps : -109.5\n",
      "Average return after 122000 timesteps : -88.7\n",
      "Average return after 124000 timesteps : -91.9\n",
      "Average return after 126000 timesteps : -115.3\n",
      "Average return after 128000 timesteps : -43.7\n",
      "Average return after 130000 timesteps : -58.8\n",
      "Average return after 132000 timesteps : -67.6\n",
      "Average return after 134000 timesteps : -36.5\n",
      "Average return after 136000 timesteps : -24.0\n",
      "Average return after 138000 timesteps : -22.9\n",
      "Average return after 140000 timesteps : -21.8\n",
      "Average return after 142000 timesteps : -13.5\n",
      "Average return after 144000 timesteps : -11.7\n",
      "Average return after 146000 timesteps : -13.9\n",
      "Average return after 148000 timesteps : -7.5\n",
      "Average return after 150000 timesteps : -15.1\n",
      "Average return after 152000 timesteps : -8.1\n",
      "Average return after 154000 timesteps : -7.2\n",
      "Average return after 156000 timesteps : -5.0\n",
      "Average return after 158000 timesteps : -27.5\n",
      "Average return after 160000 timesteps : -3.7\n",
      "Average return after 162000 timesteps : -3.1\n",
      "Average return after 164000 timesteps : -3.3\n",
      "Average return after 166000 timesteps : -4.0\n",
      "Average return after 168000 timesteps : -5.8\n",
      "Average return after 170000 timesteps : -5.5\n",
      "Average return after 172000 timesteps : -7.7\n",
      "Average return after 174000 timesteps : -8.5\n",
      "Average return after 176000 timesteps : -8.3\n",
      "Average return after 178000 timesteps : -8.6\n",
      "Average return after 180000 timesteps : -7.6\n",
      "Average return after 182000 timesteps : -4.3\n",
      "Average return after 184000 timesteps : -7.9\n",
      "Average return after 186000 timesteps : -6.6\n",
      "Average return after 188000 timesteps : -5.5\n",
      "Average return after 190000 timesteps : -6.1\n",
      "Average return after 192000 timesteps : -5.4\n",
      "Average return after 194000 timesteps : -8.8\n",
      "Average return after 196000 timesteps : -6.4\n",
      "Average return after 198000 timesteps : -9.7\n",
      "Average return after 200000 timesteps : -4.4\n",
      "Average return after 202000 timesteps : -6.6\n",
      "Average return after 204000 timesteps : -5.2\n",
      "Average return after 206000 timesteps : -6.0\n",
      "Average return after 208000 timesteps : -5.5\n",
      "Average return after 210000 timesteps : -6.4\n",
      "Average return after 212000 timesteps : -5.0\n",
      "Average return after 214000 timesteps : -11.1\n",
      "Average return after 216000 timesteps : -9.9\n",
      "Average return after 218000 timesteps : -5.0\n",
      "Average return after 220000 timesteps : -7.6\n",
      "Average return after 222000 timesteps : -7.1\n",
      "Average return after 224000 timesteps : -9.0\n",
      "Average return after 226000 timesteps : -7.6\n",
      "Average return after 228000 timesteps : -5.4\n",
      "Average return after 230000 timesteps : -5.8\n",
      "Average return after 232000 timesteps : -4.8\n",
      "Average return after 234000 timesteps : -5.9\n",
      "Average return after 236000 timesteps : -9.1\n",
      "Average return after 238000 timesteps : -8.0\n",
      "Average return after 240000 timesteps : -7.5\n",
      "Average return after 242000 timesteps : -6.2\n",
      "Average return after 244000 timesteps : -5.0\n",
      "Average return after 246000 timesteps : -6.9\n",
      "Average return after 248000 timesteps : -9.1\n",
      "Average return after 250000 timesteps : -8.8\n",
      "Average return after 252000 timesteps : -7.4\n",
      "Average return after 254000 timesteps : -6.1\n",
      "Average return after 256000 timesteps : -6.9\n",
      "Average return after 258000 timesteps : -4.5\n",
      "Average return after 260000 timesteps : -8.1\n",
      "Average return after 262000 timesteps : -7.2\n",
      "Average return after 264000 timesteps : -6.4\n",
      "Average return after 266000 timesteps : -8.5\n",
      "Average return after 268000 timesteps : -9.0\n",
      "Average return after 270000 timesteps : -9.2\n",
      "Average return after 272000 timesteps : -4.1\n",
      "Average return after 274000 timesteps : -8.8\n",
      "Average return after 276000 timesteps : -5.9\n",
      "Average return after 278000 timesteps : -7.2\n",
      "Average return after 280000 timesteps : -7.3\n",
      "Average return after 282000 timesteps : -6.4\n",
      "Average return after 284000 timesteps : -4.5\n",
      "Average return after 286000 timesteps : -8.5\n",
      "Average return after 288000 timesteps : -5.4\n",
      "Average return after 290000 timesteps : -5.4\n",
      "Average return after 292000 timesteps : -6.5\n",
      "Average return after 294000 timesteps : -5.8\n",
      "Average return after 296000 timesteps : -7.0\n",
      "Average return after 298000 timesteps : -7.1\n",
      "Average return after 300000 timesteps : -5.8\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "state, goal = obs[\"observation\"], obs[\"desired_goal\"]\n",
    "model.goal_normalizer.update(goal)\n",
    "state_goal = torch.tensor(np.concatenate([state, goal]), dtype=torch.float32, device=device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for timestep in range(timestep_start, max_timesteps):\n",
    "\n",
    "    action = model.act(state_goal)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    achieved_goal = obs[\"achieved_goal\"]\n",
    "    next_state = obs[\"observation\"]\n",
    "    model.state_normalizer.update(next_state) # Update the state normalizer with each observed state\n",
    "\n",
    "    done = terminated or truncated\n",
    "\n",
    "    model.save_to_buffer([state, goal, achieved_goal, action, reward, next_state, terminated])\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # Normalize input to network after warmup phase\n",
    "    if timestep >= warmup_timesteps:\n",
    "        state_goal = model.normalize_state_goal(state, goal) \n",
    "    else:\n",
    "        state_goal = torch.tensor(np.concatenate([state, goal]), dtype=torch.float32, device=device) \n",
    "\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "        state, goal = obs[\"observation\"], obs[\"desired_goal\"]\n",
    "        if use_her: model.save_her_transitions(achieved_goal) # Create new HER transitions using the last achieved goal as the new target\n",
    "\n",
    "        model.state_normalizer.update(state) \n",
    "        model.goal_normalizer.update(goal) # Update goal normalizer on each new target goal\n",
    "\n",
    "        # Normalize input to network after warmup phase\n",
    "        if timestep >= warmup_timesteps:\n",
    "            state_goal = model.normalize_state_goal(state, goal) \n",
    "        else:\n",
    "            state_goal = torch.tensor(np.concatenate([state, goal]), dtype=torch.float32, device=device) \n",
    "\n",
    "    if timestep >= warmup_timesteps: # Update every timestep after warmup\n",
    "        model.update(batch_size)\n",
    "\n",
    "    if (timestep + 1) % eval_frequency == 0:\n",
    "        avg_return, avg_success = eval_model(model, env_name, max_episode_steps, n_episodes_eval)\n",
    "        avg_returns.append(avg_return)\n",
    "        avg_successes.append(avg_success)\n",
    "        save_plots(avg_returns, avg_successes, f\"training/sac/{env_name}/{version}/training{training_number}\", timestep+1, eval_frequency)\n",
    "        print(f\"Average return after {timestep+1} timesteps : {avg_return}\")\n",
    "\n",
    "    if (timestep + 1) % checkpoint_frequency == 0:\n",
    "        model.save_state(timestep, avg_returns, avg_successes, f\"training/sac/{env_name}/{version}/training{training_number}/{timestep+1}.pth\")\n",
    "\n",
    "    # if (timestep + 1) % video_frequency == 0:\n",
    "    #     generate_video(env_name, model, 1, \n",
    "    #            deterministic=True, \n",
    "    #            filename=f\"training/sac/{env_name}/{version}/training{training_number}/{timestep+1}.mp4\")\n",
    "        \n",
    "    if time.time() - start_time > 3600 * max_training_time:\n",
    "        print(f\"Maximum training time of {max_training_time}h exceeded. Interrupting training after {timestep} timesteps.\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e4b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video(env_name, max_episode_steps, model, 50, random=False, deterministic=True,\n",
    "               filename=f\"training/sac/{env_name}/{version}/training{training_number}/final.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746f608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
